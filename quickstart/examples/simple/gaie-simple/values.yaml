inferenceExtension:
  replicas: 1
  image:
    name: llm-d-inference-scheduler
    hub: ghcr.io/llm-d
    tag: 0.0.4
    # name: epp
    # hub: us-central1-docker.pkg.dev/k8s-staging-images/gateway-api-inference-extension
    # tag: main
    pullPolicy: Always
  extProcPort: 9002
  env:
    ENABLE_KVCACHE_AWARE_SCORER: "false"
    ENABLE_LOAD_AWARE_SCORER: "true"
    ENABLE_PREFIX_AWARE_SCORER: "true"
    ENABLE_SESSION_AWARE_SCORER: "false"
    KVCACHE_AWARE_SCORER_WEIGHT: "1"
    LOAD_AWARE_SCORER_WEIGHT: "1"
    PD_ENABLED: "false"
    PD_PROMPT_LEN_THRESHOLD: "10"
    PREFILL_ENABLE_KVCACHE_AWARE_SCORER: "false"
    PREFILL_ENABLE_LOAD_AWARE_SCORER: "false"
    PREFILL_ENABLE_PREFIX_AWARE_SCORER: "false"
    PREFILL_ENABLE_SESSION_AWARE_SCORER: "false"
    PREFILL_KVCACHE_AWARE_SCORER_WEIGHT: "1"
    PREFILL_LOAD_AWARE_SCORER_WEIGHT: "1"
    PREFILL_PREFIX_AWARE_SCORER_WEIGHT: "1"
    PREFILL_SESSION_AWARE_SCORER_WEIGHT: "1"
    PREFIX_AWARE_SCORER_WEIGHT: "2"
    SESSION_AWARE_SCORER_WEIGHT: "1"

inferencePool:
  targetPortNumber: 8000
  modelServerType: vllm
  modelServers:
    matchLabels:
      llm-d.ai/inferenceServing: "true"
      # llm-d.ai/model: ms-simple-llm-d-modelservice
